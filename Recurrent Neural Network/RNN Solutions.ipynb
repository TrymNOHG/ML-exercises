{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning IDATT2502 - Recurrent Neural Network Exercises\n",
    "\n",
    "Info surround LSTM\n",
    "\n",
    "## Many-to-many LSTM\n",
    "Write a little about the different use-cases of such a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " hlllo     ll                                        \n",
      " hlllo world                                         \n",
      " hllo world   d           d                          \n",
      " hello world    d                                    \n",
      " hello world    dd     d     d     d     d      d    \n",
      " hello world    dd    odd    dd    odd    dd    ord  \n",
      " hello world    dd   world   ddd   orld   ddd   orld \n",
      " hello world    dd   world   ddd   world   ddll  worl\n",
      " hello world    dll  world   rdd   word   d dllo worl\n",
      " hello world    dllo world   rld  world   rld  world \n",
      " hello world   rld  world   rlll  world   rlll  world\n",
      " hello world  d dll  world   rlll  world  d hello wor\n",
      " hello world  d dllo world   rlll  world  drld  world\n",
      " hello world  d dllo world  d hello world  d hello wo\n",
      " hello world  d hllo world  d hello world wdrdd  lo w\n",
      " hello world wdrld  world wdrld  woll  world  wrld  w\n",
      " hello world world  woll  wd ld  world world  ooorld \n",
      " hello world world  woll  wd ll  world world  ooorld \n",
      " hello world world  world world  woll  world  orld  o\n",
      " hello world world  world world  wold  wdrld world wo\n",
      " hello world world  world world  wold  wdrld world wo\n",
      " hello world world  world world  wold  wdrld world wo\n",
      " hello world world  world world  world world  world w\n",
      " hello world world  world world  world world  world w\n",
      " hello world world  world world  world world  world w\n",
      " hello world world  world world  world world  world w\n",
      " hello world world  world world  world world  world w\n",
      " hello world world  world world  world world  world w\n",
      " hello world world  world world  world world  world w\n",
      " hello world world  world world  wold  wrlld world wo\n",
      " hello world world  world world  wold  wrlld world wo\n",
      " hello world world  world world  wold  wrlld world wo\n",
      " hello world world  world world  wold  wrlld world wo\n",
      " hello world world  wold  wrlld world world  world wo\n",
      " hello world world  wold  wrlld world world  world wo\n",
      " hello world world  wold  wrlld world world  world wo\n",
      " hello world world  wold  wrll  world world  world wo\n",
      " hello world world  wold  wrll  world world  world wo\n",
      " hello world world  wold  wrll  world world  world wo\n",
      " hello world world  wold  wrll  world world  world wo\n",
      " hello world world  wold  wrld  world world  world wo\n",
      " hello world world  wold  wrld  world world  wold  wr\n",
      " hello world world  wold  wrld  world world  wold  wr\n",
      " hello world world  wold  orld  wrld world  wrld worl\n",
      " hello world world  wold  orld  orld world  wrld worl\n",
      " hello world world  wold  orld world world  world wor\n",
      " hello world world  wold  orld world world  world wor\n",
      " hello world world  wold world world world world  orl\n",
      " hello world world  wold world world world world  wrl\n",
      " hello world world  wrld world world  orld world worl\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class LongShortTermMemoryModel(nn.Module):\n",
    "\n",
    "    def __init__(self, encoding_size):\n",
    "        super(LongShortTermMemoryModel, self).__init__()\n",
    "\n",
    "        self.lstm = nn.LSTM(encoding_size, 128)  # 128 is the state size\n",
    "        self.dense = nn.Linear(128, encoding_size)  # 128 is the state size\n",
    "\n",
    "    def reset(self):  # Reset states prior to new input sequence\n",
    "        zero_state = torch.zeros(1, 1, 128)  # Shape: (number of layers, batch size, state size)\n",
    "        self.hidden_state = zero_state\n",
    "        self.cell_state = zero_state\n",
    "\n",
    "    def logits(self, x):  # x shape: (sequence length, batch size, encoding size)\n",
    "        out, (self.hidden_state, self.cell_state) = self.lstm(x, (self.hidden_state, self.cell_state))\n",
    "        return self.dense(out.reshape(-1, 128))\n",
    "\n",
    "    def f(self, x):  # x shape: (sequence length, batch size, encoding size)\n",
    "        return torch.softmax(self.logits(x), dim=1)\n",
    "\n",
    "    def loss(self, x, y):  # x shape: (sequence length, batch size, encoding size), y shape: (sequence length, encoding size)\n",
    "        return nn.functional.cross_entropy(self.logits(x), y.argmax(1))\n",
    "\n",
    "\n",
    "char_encodings = [\n",
    "    [1., 0., 0., 0., 0., 0., 0., 0.],  # ' '\n",
    "    [0., 1., 0., 0., 0., 0., 0., 0.],  # 'h'\n",
    "    [0., 0., 1., 0., 0., 0., 0., 0.],  # 'e'\n",
    "    [0., 0., 0., 1., 0., 0., 0., 0.],  # 'l'\n",
    "    [0., 0., 0., 0., 1., 0., 0., 0.],  # 'o'\n",
    "    [0., 0., 0., 0., 0., 1., 0., 0.],  # 'w'\n",
    "    [0., 0., 0., 0., 0., 0., 1., 0.],  # 'r'\n",
    "    [0., 0., 0., 0., 0., 0., 0., 1.],  # 'd'\n",
    "]\n",
    "encoding_size = len(char_encodings)\n",
    "\n",
    "index_to_char = [' ', 'h', 'e', 'l', 'o', 'w', 'r', 'd']\n",
    "\n",
    "x_train = torch.tensor([[char_encodings[0]], [char_encodings[1]], [char_encodings[2]], [char_encodings[3]], [char_encodings[3]],\n",
    "                        [char_encodings[4]], [char_encodings[0]], [char_encodings[5]], [char_encodings[4]], [char_encodings[6]], [char_encodings[3]], [char_encodings[7]]])  # ' hello world'\n",
    "y_train = torch.tensor([char_encodings[1], char_encodings[2], char_encodings[3], char_encodings[3], char_encodings[4], char_encodings[0], char_encodings[5], char_encodings[4], char_encodings[6], char_encodings[3], char_encodings[7], char_encodings[0]])  # 'hello world '\n",
    "\n",
    "model = LongShortTermMemoryModel(encoding_size)\n",
    "\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), 0.001)\n",
    "for epoch in range(500):\n",
    "    model.reset()\n",
    "    model.loss(x_train, y_train).backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if epoch % 10 == 9:\n",
    "        # Generate characters from the initial characters ' h'\n",
    "        model.reset()\n",
    "        text = ' h'\n",
    "        model.f(torch.tensor([[char_encodings[0]]]))\n",
    "        y = model.f(torch.tensor([[char_encodings[1]]]))\n",
    "        text += index_to_char[y.argmax(1)]\n",
    "        for c in range(50):\n",
    "            y = model.f(torch.tensor([[char_encodings[y.argmax(1)]]]))\n",
    "            text += index_to_char[y.argmax(1)]\n",
    "        print(text)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Many-to-one LSTM\n",
    "Write a little about the different use cases of such a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class LongShortTermMemoryModel(nn.Module):\n",
    "\n",
    "    def __init__(self, encoding_size, emoji_len):\n",
    "        super(LongShortTermMemoryModel, self).__init__()\n",
    "\n",
    "        self.lstm = nn.LSTM(encoding_size, 128)  # 128 is the state size\n",
    "        self.dense = nn.Linear(128, emoji_len)  # 128 is the state size\n",
    "\n",
    "    def reset(self):  # Reset states prior to new input sequence\n",
    "        zero_state = torch.zeros(1, 1, 128)  # Shape: (number of layers, batch size, state size)\n",
    "        self.hidden_state = zero_state\n",
    "        self.cell_state = zero_state\n",
    "\n",
    "    def logits(self, x):  # x shape: (sequence length, batch size, encoding size)\n",
    "        out, (self.hidden_state, self.cell_state) = self.lstm(x, (self.hidden_state, self.cell_state))\n",
    "        return self.dense(out.reshape(-1, 128))\n",
    "\n",
    "    def f(self, x):  # x shape: (sequence length, batch size, encoding size)\n",
    "        return torch.softmax(self.logits(x), dim=1)\n",
    "\n",
    "    def loss(self, x, y):  # x shape: (sequence length, batch size, encoding size), y shape: (sequence length, encoding size)\n",
    "        return nn.functional.cross_entropy(self.logits(x), y.argmax(1))\n",
    "\n",
    "\n",
    "char_encodings = [\n",
    "    [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],  # ' '\n",
    "    [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],  # 'a'\n",
    "    [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],  # 'b'\n",
    "    [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],  # 'c'\n",
    "    [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],  # 'd'\n",
    "    [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],  # 'e'\n",
    "    [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],  # 'f'\n",
    "    [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],  # 'g'\n",
    "    [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],  # 'h'\n",
    "    [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],  # 'i'\n",
    "    [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],  # 'j'\n",
    "    [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],  # 'k'\n",
    "    [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],  # 'l'\n",
    "    [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],  # 'm'\n",
    "    [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],  # 'n'\n",
    "    [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],  # 'o'\n",
    "    [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],  # 'p'\n",
    "    [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],  # 'q'\n",
    "    [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],  # 'r'\n",
    "    [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],  # 's'\n",
    "    [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],  # 't'\n",
    "    [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],  # 'u'\n",
    "    [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],  # 'v'\n",
    "    [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],  # 'w'\n",
    "    [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],  # 'x'\n",
    "    [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],  # 'y'\n",
    "    [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],  # 'z'\n",
    "\n",
    "]\n",
    "\n",
    "emoji = {\n",
    "    'hat ': '\\U0001F3A9',\n",
    "    'rat ': '\\U0001F400',\n",
    "    'cat ': '\\U0001F408',\n",
    "    'flat': '\\U0001F3E2',\n",
    "    'matt': '\\U0001F468',\n",
    "    'cap ': '\\U0001F9E2',\n",
    "    'son ': '\\U0001F466'\n",
    "}\n",
    "\n",
    "emoji_encodings = [\n",
    "    [1., 0., 0., 0., 0., 0., 0.], # hat\n",
    "    [0., 1., 0., 0., 0., 0., 0.], # rat\n",
    "    [0., 0., 1., 0., 0., 0., 0.], # cat\n",
    "    [0., 0., 0., 1., 0., 0., 0.], # flat\n",
    "    [0., 0., 0., 0., 1., 0., 0.], # matt\n",
    "    [0., 0., 0., 0., 0., 1., 0.], # cap\n",
    "    [0., 0., 0., 0., 0., 0., 1.], # son\n",
    "]\n",
    "\n",
    "\n",
    "encoding_size = len(char_encodings)\n",
    "emoji_len= len(emoji_encodings)\n",
    "\n",
    "index_to_char = [' ', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
    "index_to_emoji =[emoji_utf for emoji_utf in emoji.items()]\n",
    "\n",
    "words = ['hat ', 'rat ', 'cat ', 'flat', 'matt', 'cap ', 'son ']\n",
    "\n",
    "max_emoji_len = max(len(word) for word in words)\n",
    "\n",
    "x_train = [[[char_encodings[index_to_char.index(letter)]] for letter in word] for word in words]\n",
    "y_train = [[emoji_encoding for i in range(max_emoji_len)] for emoji_encoding in emoji_encodings]\n",
    "\n",
    "\n",
    "\n",
    "x_train = torch.tensor(x_train)\n",
    "y_train = torch.tensor(y_train)\n",
    "\n",
    "model = LongShortTermMemoryModel(encoding_size, emoji_len)\n",
    "\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), 0.001)\n",
    "for epoch in range(500):\n",
    "    for i in range(x_train.size()[0]):\n",
    "        model.reset()\n",
    "        model.loss(x_train[i], y_train[i]).backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "\n",
    "def predict_emoji(text):\n",
    "    y = 0\n",
    "    model.reset()\n",
    "    for letter in text:\n",
    "        x = torch.tensor([[char_encodings[index_to_char.index(letter)]]])\n",
    "        y =  model.f(x)\n",
    "    print(index_to_emoji[y.argmax(1)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('rat ', 'üêÄ')\n",
      "('rat ', 'üêÄ')\n",
      "('matt', 'üë®')\n",
      "('cap ', 'üß¢')\n",
      "('rat ', 'üêÄ')\n",
      "('hat ', 'üé©')\n",
      "('flat', 'üè¢')\n"
     ]
    }
   ],
   "source": [
    "predict_emoji('rt')\n",
    "predict_emoji('rat')\n",
    "predict_emoji('at')\n",
    "predict_emoji('ca')\n",
    "predict_emoji('rts')\n",
    "predict_emoji('hat')\n",
    "predict_emoji('fl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
